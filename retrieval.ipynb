{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e63597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import logging, os, re, sys, pickle, json, hashlib, uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_community.document_loaders import (\n",
    "        PyMuPDFLoader,\n",
    "        Docx2txtLoader,\n",
    "        UnstructuredPowerPointLoader,\n",
    "        TextLoader,\n",
    "        UnstructuredMarkdownLoader,\n",
    "    )\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "except Exception as e:\n",
    "    print(\"This script requires langchain and compatible loaders.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SBERT_OK = True\n",
    "except Exception:\n",
    "    _SBERT_OK = False\n",
    "\n",
    "# [MODIFIED] Import for Reranker\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    _CROSS_ENCODER_OK = True\n",
    "except Exception:\n",
    "    _CROSS_ENCODER_OK = False\n",
    "\n",
    "\n",
    "# --- Logging ---\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_DIR / \"ingest.log\", mode=\"a\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(\"rag.ingest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IngestConfig:\n",
    "    root_dir: Path\n",
    "    subject: str = \"General\"\n",
    "    chunk_sizes: Tuple[int, ...] = (2048, 512, 128)\n",
    "    overlap_ratio: float = 0.12\n",
    "    max_overlap: int = 220\n",
    "    models: Tuple[str, ...] = (\"all-mpnet-base-v2\", \"BAAI/bge-base-en-v1.5\")\n",
    "    reranker_model: Optional[str] = \"BAAI/bge-reranker-base\"\n",
    "\n",
    "\n",
    "LOADER_MAP = {\n",
    "    \".pdf\": PyMuPDFLoader,\n",
    "    \".docx\": Docx2txtLoader,\n",
    "    \".pptx\": UnstructuredPowerPointLoader,\n",
    "    \".txt\": TextLoader,\n",
    "    \".md\": UnstructuredMarkdownLoader,\n",
    "}\n",
    "\n",
    "STOPWORDS = set(\"\"\"\n",
    "a an the and or but if then else when while at by for with about against between\n",
    "into through during before after above below to from up down in out on off over under\n",
    "again further then once here there all any both each few more most other some such\n",
    "no nor not only own same so than too very s t can will just don should now is are\n",
    "am was were be been being of this that these those it its as i you he she they we\n",
    "me him her them my your their our\n",
    "\"\"\".split())\n",
    "\n",
    "_ws_re = re.compile(r\"\\s+\")\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "_noninformative_re = re.compile(r\"^[\\s\\-\\*\\d\\.|]+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return _ws_re.sub(\" \", text).strip()\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = _url_re.sub(\" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    return [t for t in text.split() if t and t not in STOPWORDS]\n",
    "\n",
    "def remove_noninformative(text: str) -> str:\n",
    "    kept = []\n",
    "    for line in text.splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if _noninformative_re.match(line.strip()):\n",
    "            continue\n",
    "        kept.append(line)\n",
    "    return \"\\n\".join(kept)\n",
    "\n",
    "def preprocess_text(raw: str) -> str:\n",
    "    return normalize_whitespace(remove_noninformative(raw).lower())\n",
    "\n",
    "\n",
    "def jaccard(a: Iterable[str], b: Iterable[str]) -> float:\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb:\n",
    "        return 1.0\n",
    "    return len(sa & sb) / max(1, len(sa | sb))\n",
    "\n",
    "def dedup_documents(docs: List[Document], near_threshold: float = 0.92) -> List[Document]:\n",
    "    seen_hashes, kept, token_cache = set(), [], []\n",
    "    for doc in docs:\n",
    "        txt = doc.page_content\n",
    "        h = hash(txt)\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        tokens = simple_tokenize(txt)\n",
    "        if any(jaccard(tokens, toks) >= near_threshold for toks in token_cache):\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "        kept.append(doc)\n",
    "        token_cache.append(tokens)\n",
    "    return kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, cfg: IngestConfig):\n",
    "        self.cfg = cfg\n",
    "        self._docid_by_path: Dict[str, str] = {}\n",
    "        self.cfg.chunk_sizes = tuple(sorted(cfg.chunk_sizes, reverse=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def _stable_id(seed: str, prefix: str) -> str:\n",
    "        h = hashlib.sha1(seed.encode(\"utf-8\")).hexdigest()[:16]\n",
    "        return f\"{prefix}-{h}\"\n",
    "\n",
    "    def _loader_for(self, path: Path):\n",
    "        ext = path.suffix.lower()\n",
    "        if ext in LOADER_MAP:\n",
    "            return LOADER_MAP[ext]\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "\n",
    "    def _augment_metadata(self, doc: Document, file_path: Path, seq_in_file: int):\n",
    "        file_key = str(file_path.resolve())\n",
    "        if file_key not in self._docid_by_path:\n",
    "            self._docid_by_path[file_key] = self._stable_id(file_key, \"doc\")\n",
    "        doc_id = self._docid_by_path[file_key]\n",
    "\n",
    "        doc.metadata.update({\n",
    "            \"subject\": self.cfg.subject,\n",
    "            \"source\": file_path.name,\n",
    "            \"context\": doc.page_content[:200],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"file_path\": file_key,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"doc_seq\": seq_in_file,\n",
    "        })\n",
    "\n",
    "    def _preprocess_doc(self, doc: Document) -> Document:\n",
    "        doc.page_content = preprocess_text(doc.page_content)\n",
    "        return doc\n",
    "\n",
    "    def load_all(self) -> List[Document]:\n",
    "        all_docs = []\n",
    "        for root, _, files in os.walk(self.cfg.root_dir):\n",
    "            for fname in files:\n",
    "                path = Path(root) / fname\n",
    "                try:\n",
    "                    if path.suffix.lower() in LOADER_MAP:\n",
    "                        loader = self._loader_for(path)(str(path))\n",
    "                        docs = loader.load()\n",
    "                        for i, d in enumerate(docs):\n",
    "                            self._augment_metadata(d, path, i)\n",
    "                            self._preprocess_doc(d)\n",
    "                        all_docs.extend(docs)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {path}: {e}\")\n",
    "        deduped = dedup_documents(all_docs)\n",
    "        logger.info(f\"Loaded {len(deduped)} unique documents (deduped from {len(all_docs)})\")\n",
    "        return deduped\n",
    "\n",
    "    def hierarchical_chunks(self, documents: List[Document]) -> Dict[int, List[Document]]:\n",
    "        results: Dict[int, List[Document]] = {}\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "\n",
    "        if not self.cfg.chunk_sizes:\n",
    "            return {}\n",
    "\n",
    "        parent_size = self.cfg.chunk_sizes[0]\n",
    "        overlap = min(int(parent_size * self.cfg.overlap_ratio), self.cfg.max_overlap)\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=parent_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=separators,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        parent_chunks: List[Document] = []\n",
    "        for d in documents:\n",
    "            original_doc_id = d.metadata.get(\"doc_id\", self._stable_id(d.metadata.get(\"file_path\",\"\") or uuid.uuid4().hex, \"doc\"))\n",
    "            \n",
    "            split = splitter.split_documents([d])\n",
    "            for i, ch in enumerate(split):\n",
    "                start = ch.metadata.get(\"start_index\", 0)\n",
    "                seed = f\"{original_doc_id}|{parent_size}|{i}|{start}|{len(ch.page_content)}\"\n",
    "                chunk_id = self._stable_id(seed, \"chunk\")\n",
    "                \n",
    "                ch.metadata.update({\n",
    "                    \"chunk_size\": parent_size,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"level\": parent_size,\n",
    "                    \"parent_doc_id\": original_doc_id, \n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"parent_chunk_id\": None, \n",
    "                    **{k: v for k, v in d.metadata.items() if k not in ch.metadata},\n",
    "                })\n",
    "                parent_chunks.append(ch)\n",
    "\n",
    "        pruned_parents = dedup_documents(parent_chunks)\n",
    "        logger.info(f\"Chunk size={parent_size}: {len(pruned_parents)} unique chunks (overlap={overlap})\")\n",
    "        results[parent_size] = pruned_parents\n",
    "        \n",
    "        previous_level_chunks = pruned_parents \n",
    "\n",
    "        for size in self.cfg.chunk_sizes[1:]: \n",
    "            overlap = min(int(size * self.cfg.overlap_ratio), self.cfg.max_overlap)\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=size,\n",
    "                chunk_overlap=overlap,\n",
    "                separators=separators,\n",
    "                add_start_index=True,\n",
    "            )\n",
    "            \n",
    "            current_level_chunks: List[Document] = []\n",
    "            \n",
    "            for parent_chunk in previous_level_chunks:\n",
    "                parent_chunk_id = parent_chunk.metadata[\"chunk_id\"]\n",
    "                original_doc_id = parent_chunk.metadata[\"parent_doc_id\"] \n",
    "\n",
    "                parent_doc_for_splitting = Document(\n",
    "                    page_content=parent_chunk.page_content\n",
    "                )\n",
    "                split = splitter.split_documents([parent_doc_for_splitting]) \n",
    "                \n",
    "                for i, ch in enumerate(split):\n",
    "                    start = ch.metadata.get(\"start_index\", 0)\n",
    "                    seed = f\"{parent_chunk_id}|{size}|{i}|{start}|{len(ch.page_content)}\"\n",
    "                    chunk_id = self._stable_id(seed, \"chunk\")\n",
    "                    \n",
    "                    new_metadata = parent_chunk.metadata.copy()\n",
    "                    \n",
    "                    new_metadata.update({\n",
    "                        \"chunk_size\": size,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"level\": size,\n",
    "                        \"parent_doc_id\": original_doc_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"parent_chunk_id\": parent_chunk_id, \n",
    "                        \"start_index\": start,\n",
    "                    })\n",
    "                    \n",
    "                    child_chunk = Document(\n",
    "                        page_content=ch.page_content,\n",
    "                        metadata=new_metadata\n",
    "                    )\n",
    "                    current_level_chunks.append(child_chunk)\n",
    "\n",
    "            pruned_children = dedup_documents(current_level_chunks)\n",
    "            logger.info(f\"Chunk size={size}: {len(pruned_children)} unique chunks (overlap={overlap})\")\n",
    "            results[size] = pruned_children\n",
    "            \n",
    "            previous_level_chunks = pruned_children \n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaissVectorStore:\n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = int(dimension)\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.documents: List[Document] = []\n",
    "\n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "        self.documents.extend(documents)\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(self.index, str(path / \"index.faiss\"))\n",
    "        with open(path / \"docs.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.documents, f)\n",
    "        manifest = []\n",
    "        for d in self.documents:\n",
    "            md = d.metadata\n",
    "            manifest.append({\n",
    "                \"chunk_id\": md.get(\"chunk_id\"),\n",
    "                \"parent_chunk_id\": md.get(\"parent_chunk_id\"),\n",
    "                \"parent_doc_id\": md.get(\"parent_doc_id\"),\n",
    "                \"doc_id\": md.get(\"doc_id\"),\n",
    "                \"source\": md.get(\"source\"),\n",
    "                \"chunk_size\": md.get(\"chunk_size\"),\n",
    "                \"chunk_index\": md.get(\"chunk_index\"),\n",
    "                \"file_path\": md.get(\"file_path\"),\n",
    "                \"subject\": md.get(\"subject\"),\n",
    "            })\n",
    "        with open(path / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> \"FaissVectorStore\":\n",
    "        index = faiss.read_index(str(path / \"index.faiss\"))\n",
    "        with open(path / \"docs.pkl\", \"rb\") as f:\n",
    "            docs = pickle.load(f)\n",
    "        obj = cls(index.d)\n",
    "        obj.index, obj.documents = index, docs\n",
    "        return obj\n",
    "\n",
    "    def similarity_search(self, query_embedding: np.ndarray, k: int = 4):\n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        k = min(k, self.index.ntotal)\n",
    "        if k == 0:\n",
    "            return [], []\n",
    "        D, I = self.index.search(query_embedding.astype(np.float32), k)\n",
    "        return [self.documents[i] for i in I[0]], [float(d) for d in D[0]]\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_names: Iterable[str]):\n",
    "        if not _SBERT_OK:\n",
    "            raise RuntimeError(\"sentence-transformers is required\")\n",
    "        self.models = {m: SentenceTransformer(m) for m in model_names}\n",
    "\n",
    "    def encode(self, model: str, texts: List[str]) -> np.ndarray:\n",
    "        return np.array(self.models[model].encode(texts, batch_size=32, show_progress_bar=False))\n",
    "\n",
    "    def dim(self, model: str) -> int:\n",
    "        vec = self.encode(model, [\"\"])\n",
    "        return int(vec.shape[1])\n",
    "\n",
    "\n",
    "class Reranker:\n",
    "    def __init__(self, model_name: str, batch_size: int = 8):\n",
    "        self.model_name = model_name\n",
    "        if not _CROSS_ENCODER_OK:\n",
    "            logger.error(\"CrossEncoder not found. Reranking will be disabled.\")\n",
    "            self.model = None\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.model = CrossEncoder(model_name)\n",
    "            self.batch_size = batch_size\n",
    "            logger.info(f\"Loaded Reranker model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load Reranker model {model_name}. Reranking disabled. Error: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = 5) -> List[Document]:\n",
    "        if not self.model or not documents:\n",
    "            return documents[:top_k]\n",
    "\n",
    "        pairs = [(query, doc.page_content) for doc in documents]\n",
    "        scores = self.model.predict(pairs, batch_size=self.batch_size, show_progress_bar=False)\n",
    "        \n",
    "        scored_docs = list(zip(scores, documents))\n",
    "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return [doc for score, doc in scored_docs[:top_k]]\n",
    "\n",
    "\n",
    "class RAGIndexer:\n",
    "    def __init__(self, cfg: IngestConfig):\n",
    "        self.cfg = cfg\n",
    "        self.embed_mgr = EmbeddingManager(cfg.models)\n",
    "        self.vectorstores: Dict[Tuple[str, int], FaissVectorStore] = {}\n",
    "\n",
    "    def build_indexes(self, chunks_by_size: Dict[int, List[Document]], save_dir: Path):\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for model_name in self.cfg.models:\n",
    "            dim = self.embed_mgr.dim(model_name)\n",
    "            for size, docs in chunks_by_size.items():\n",
    "                if not docs:\n",
    "                    continue\n",
    "                logger.info(f\"Embedding {len(docs)} chunks (size={size}) with {model_name}\")\n",
    "                embeddings = self.embed_mgr.encode(model_name, [d.page_content for d in docs])\n",
    "                vs = FaissVectorStore(dimension=dim)\n",
    "                vs.add_documents(docs, embeddings)\n",
    "                \n",
    "                model_path_name = re.sub(r'[^a-zA-Z0-9_-]', '', model_name)\n",
    "                out_path = save_dir / f\"{model_path_name}_{size}\"\n",
    "                \n",
    "                vs.save(out_path)\n",
    "                self.vectorstores[(model_name, size)] = vs\n",
    "                logger.info(f\"Saved FAISS index -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_search(\n",
    "    query: str,\n",
    "    indexer: RAGIndexer,\n",
    "    embed_mgr: EmbeddingManager,\n",
    "    model_name: str = \"all-mpnet-base-v2\",\n",
    "    reranker: Optional[Reranker] = None,\n",
    "    top_k_parent: int = 3,\n",
    "    top_k_intermediate: int = 5,\n",
    "    top_k_final: int = 5,       \n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    MODIFIED: Hierarchical retrieval (Top-Down) with Reranker at the final step.\n",
    "    1. Search top-level chunks (e.g., 2048).\n",
    "    2. Find all child chunks (e.g., 512) of those top hits.\n",
    "    3. Re-rank (Vector) the child chunks against the query.\n",
    "    4. Find all grand-child chunks (e.g., 128) of those new hits.\n",
    "    5. Re-rank (CrossEncoder) the grand-child chunks and return them.\n",
    "    \"\"\"\n",
    "    q_emb = embed_mgr.encode(model_name, [query])[0]\n",
    "    q_emb_2d = q_emb.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "    sizes = sorted(indexer.cfg.chunk_sizes, reverse=True)\n",
    "    \n",
    "    coarse_size = sizes[0]\n",
    "    coarse_vs = indexer.vectorstores.get((model_name, coarse_size))\n",
    "    if coarse_vs is None:\n",
    "        raise ValueError(f\"No {coarse_size}-level index found.\")\n",
    "        \n",
    "    coarse_hits, _ = coarse_vs.similarity_search(q_emb, k=top_k_parent)\n",
    "    parent_chunk_ids = {h.metadata[\"chunk_id\"] for h in coarse_hits}\n",
    "\n",
    "    if len(sizes) < 2:\n",
    "        return coarse_hits \n",
    "\n",
    "    medium_size = sizes[1]\n",
    "    medium_vs = indexer.vectorstores.get((model_name, medium_size))\n",
    "    if medium_vs is None:\n",
    "        raise ValueError(f\"No {medium_size}-level index found.\")\n",
    "        \n",
    "    medium_candidates = [\n",
    "        d for d in medium_vs.documents \n",
    "        if d.metadata.get(\"parent_chunk_id\") in parent_chunk_ids\n",
    "    ]\n",
    "    \n",
    "    if not medium_candidates:\n",
    "        logger.warning(f\"No medium-level ({medium_size}) candidates found. Returning coarse hits.\")\n",
    "        return coarse_hits\n",
    "\n",
    "    medium_texts = [d.page_content for d in medium_candidates]\n",
    "    medium_embs = embed_mgr.encode(model_name, medium_texts).astype(np.float32)\n",
    "    \n",
    "    medium_rerank_index = faiss.IndexFlatL2(medium_embs.shape[1])\n",
    "    medium_rerank_index.add(medium_embs)\n",
    "    k_medium = min(top_k_intermediate, len(medium_candidates))\n",
    "    D, I = medium_rerank_index.search(q_emb_2d, k=k_medium)\n",
    "    \n",
    "    medium_hits = [medium_candidates[i] for i in I[0]]\n",
    "    child_chunk_ids = {h.metadata[\"chunk_id\"] for h in medium_hits}\n",
    "\n",
    "    if len(sizes) < 3:\n",
    "        return medium_hits \n",
    "\n",
    "    fine_size = sizes[2]\n",
    "    fine_vs = indexer.vectorstores.get((model_name, fine_size))\n",
    "    if fine_vs is None:\n",
    "        logger.info(f\"No fine-level ({fine_size}) index found. Returning medium hits.\")\n",
    "        return medium_hits\n",
    "\n",
    "    fine_candidates = [\n",
    "        d for d in fine_vs.documents \n",
    "        if d.metadata.get(\"parent_chunk_id\") in child_chunk_ids\n",
    "    ]\n",
    "    \n",
    "    if not fine_candidates:\n",
    "        logger.warning(f\"No fine-level ({fine_size}) candidates found. Returning medium hits.\")\n",
    "        return medium_hits\n",
    "\n",
    "    if reranker and reranker.model:\n",
    "        logger.info(f\"Reranking {len(fine_candidates)} fine candidates with {reranker.model_name}...\")\n",
    "        fine_hits = reranker.rerank(query, fine_candidates, top_k=top_k_final)\n",
    "    \n",
    "    else:\n",
    "        if reranker:\n",
    "            logger.warning(\"Reranker provided but model not loaded. Falling back to vector re-rank.\")\n",
    "        else:\n",
    "            logger.info(f\"No reranker. Using vector similarity for final re-rank.\")\n",
    "            \n",
    "        fine_texts = [d.page_content for d in fine_candidates]\n",
    "        fine_embs = embed_mgr.encode(model_name, fine_texts).astype(np.float32)\n",
    "\n",
    "        fine_rerank_index = faiss.IndexFlatL2(fine_embs.shape[1])\n",
    "        fine_rerank_index.add(fine_embs)\n",
    "        k_fine = min(top_k_final, len(fine_candidates))\n",
    "        D, I = fine_rerank_index.search(q_emb_2d, k=k_fine)\n",
    "        \n",
    "        fine_hits = [fine_candidates[i] for i in I[0]]\n",
    "    \n",
    "    return fine_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ce0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = IngestConfig(\n",
    "    root_dir=Path(r\"C:\\Users\\vsai2\\Documents\\LAMA\\Major Project\\Vals\\Docs\"),\n",
    "    subject=\"Geography - Natural Resources\",\n",
    "    models=(\"all-mpnet-base-v2\",\"BAAI/bge-base-en-v1.5\"), \n",
    "    chunk_sizes=(2048, 512, 128),\n",
    "    reranker_model=\"BAAI/bge-reranker-base\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 20:59:31,209 [INFO] rag.ingest: Loaded 440 unique documents (deduped from 440)\n",
      "2025-10-25 20:59:34,172 [INFO] rag.ingest: Chunk size=2048: 730 unique chunks (overlap=220)\n",
      "2025-10-25 20:59:48,188 [INFO] rag.ingest: Chunk size=512: 2476 unique chunks (overlap=61)\n",
      "2025-10-25 21:02:11,705 [INFO] rag.ingest: Chunk size=128: 9752 unique chunks (overlap=15)\n",
      "2025-10-25 21:02:11,708 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:02:11,708 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2025-10-25 21:02:15,422 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:02:15,422 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:02:19,385 [INFO] rag.ingest: Embedding 730 chunks (size=2048) with all-mpnet-base-v2\n",
      "2025-10-25 21:03:16,018 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\all-mpnet-base-v2_2048\n",
      "2025-10-25 21:03:16,019 [INFO] rag.ingest: Embedding 2476 chunks (size=512) with all-mpnet-base-v2\n",
      "2025-10-25 21:04:31,518 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\all-mpnet-base-v2_512\n",
      "2025-10-25 21:04:31,519 [INFO] rag.ingest: Embedding 9752 chunks (size=128) with all-mpnet-base-v2\n",
      "2025-10-25 21:06:02,920 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\all-mpnet-base-v2_128\n",
      "2025-10-25 21:06:03,019 [INFO] rag.ingest: Embedding 730 chunks (size=2048) with BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:07:09,448 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\BAAIbge-base-en-v15_2048\n",
      "2025-10-25 21:07:09,448 [INFO] rag.ingest: Embedding 2476 chunks (size=512) with BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:08:25,023 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\BAAIbge-base-en-v15_512\n",
      "2025-10-25 21:08:25,024 [INFO] rag.ingest: Embedding 9752 chunks (size=128) with BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:09:53,251 [INFO] rag.ingest: Saved FAISS index -> faiss_store_nested\\BAAIbge-base-en-v15_128\n",
      "2025-10-25 21:09:53,258 [INFO] rag.ingest: ✅ Vector stores built and saved.\n"
     ]
    }
   ],
   "source": [
    "docproc = DocumentProcessor(cfg)\n",
    "docs = docproc.load_all()\n",
    "\n",
    "chunks_by_size = docproc.hierarchical_chunks(docs)\n",
    "\n",
    "indexer = RAGIndexer(cfg)\n",
    "save_dir = Path(\"faiss_store_nested\") \n",
    "indexer.build_indexes(chunks_by_size, save_dir)\n",
    "logger.info(\"✅ Vector stores built and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15834c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 21:11:43,797 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:11:43,798 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2025-10-25 21:11:49,260 [INFO] sentence_transformers.cross_encoder.CrossEncoder: Use pytorch device: cpu\n",
      "2025-10-25 21:11:49,514 [INFO] rag.ingest: Loaded Reranker model: BAAI/bge-reranker-base\n",
      "2025-10-25 21:11:49,903 [INFO] rag.ingest: Reranking 23 fine candidates with BAAI/bge-reranker-base...\n",
      "\n",
      "--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\n",
      "--- (Reranked final 128-level with BAAI/bge-reranker-base) ---\n",
      "[1] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-cbc9641e7d6a10cd\n",
      "  parent_chunk=chunk-068d2c0b45e01b6d\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . physical changes and pollution of land, soil, water and air associated with mining operations, directly or indirectly affects ...\n",
      "\n",
      "[2] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-abdd53b6ad19918a\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  water directly kills living organisms; mining operations causes indirect impacts such as change is soil nutrient cycling, ...\n",
      "\n",
      "[3] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-d238a44d6257d081\n",
      "  parent_chunk=chunk-f53895edabc51b4a\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  through sediments, increased pollution, and reduced water quality ...\n",
      "\n",
      "[4] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-df1407a7c097d0d0\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . for example mining activity causes removal and destruction of forest resources; contact with toxic soil or water directly ...\n",
      "\n",
      "[5] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-9a6d12bf11b9688b\n",
      "  parent_chunk=chunk-07f74009d1e0d31c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . d. social impacts associated with large-scale mining activities; result from a rapid influx of workers into mining sites ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = cfg.models[0]\n",
    "embed_mgr = EmbeddingManager([model_name]) \n",
    "query = \"Impact of mining on the environment\"\n",
    "\n",
    "reranker = None\n",
    "if cfg.reranker_model:\n",
    "    if not _CROSS_ENCODER_OK:\n",
    "        logger.warning(\"CrossEncoder not found. Reranking will be disabled.\")\n",
    "    else:\n",
    "        reranker = Reranker(cfg.reranker_model)\n",
    "        if not reranker.model:\n",
    "            reranker = None\n",
    "\n",
    "try:\n",
    "    hits = hierarchical_search(\n",
    "        query=query,\n",
    "        indexer=indexer,\n",
    "        embed_mgr=embed_mgr,\n",
    "        model_name=model_name,\n",
    "        reranker=reranker,        \n",
    "        top_k_parent=3,          \n",
    "        top_k_intermediate=5,    \n",
    "        top_k_final=5,            \n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\")\n",
    "    if reranker:\n",
    "        print(f\"--- (Reranked final 128-level with {cfg.reranker_model}) ---\")\n",
    "    else:\n",
    "        print(\"--- (No Reranker Used - Fallback to Vector Search) ---\")\n",
    "\n",
    "    for i, d in enumerate(hits):\n",
    "        print(f\"[{i+1}] {d.metadata.get('source', '?')} | chunk_size={d.metadata.get('chunk_size')}\")\n",
    "        print(f\"  chunk_id={d.metadata.get('chunk_id')}\")\n",
    "        print(f\"  parent_chunk={d.metadata.get('parent_chunk_id')}\")\n",
    "        print(f\"  parent_doc={d.metadata.get('parent_doc_id')}\")\n",
    "        print(\"  \" + d.page_content[:250].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    logger.error(f\"Search failed: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 21:12:05,694 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:12:05,695 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:12:11,077 [INFO] sentence_transformers.cross_encoder.CrossEncoder: Use pytorch device: cpu\n",
      "2025-10-25 21:12:11,331 [INFO] rag.ingest: Loaded Reranker model: BAAI/bge-reranker-base\n",
      "2025-10-25 21:12:11,725 [INFO] rag.ingest: Reranking 20 fine candidates with BAAI/bge-reranker-base...\n",
      "\n",
      "--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\n",
      "--- (Reranked final 128-level with BAAI/bge-reranker-base) ---\n",
      "[1] 1587401677_BA(H)-Psc-Eco-Eng-BA(P)-II-Natural_Resource.pdf | chunk_size=128\n",
      "  chunk_id=chunk-63666a816fd7af7f\n",
      "  parent_chunk=chunk-8aabd4e5f06bcf4e\n",
      "  parent_doc=doc-7b887870a0f03587\n",
      "  . mining major effects of mining operations on forest and tribal people are: • mining from shallow deposits is done by surface ...\n",
      "\n",
      "[2] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-abdd53b6ad19918a\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  water directly kills living organisms; mining operations causes indirect impacts such as change is soil nutrient cycling, ...\n",
      "\n",
      "[3] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-d238a44d6257d081\n",
      "  parent_chunk=chunk-f53895edabc51b4a\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  through sediments, increased pollution, and reduced water quality ...\n",
      "\n",
      "[4] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-df1407a7c097d0d0\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . for example mining activity causes removal and destruction of forest resources; contact with toxic soil or water directly ...\n",
      "\n",
      "[5] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-2acc9d139ffbc3e5\n",
      "  parent_chunk=chunk-6b6f173f37e914a8\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . removal and disturbance of the land cover and materials in the mining area large scale mining operations disturb the land by ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = cfg.models[1]\n",
    "embed_mgr = EmbeddingManager([model_name]) \n",
    "query = \"Impact of mining on the environment\"\n",
    "\n",
    "reranker = None\n",
    "if cfg.reranker_model:\n",
    "    if not _CROSS_ENCODER_OK:\n",
    "        logger.warning(\"CrossEncoder not found. Reranking will be disabled.\")\n",
    "    else:\n",
    "        reranker = Reranker(cfg.reranker_model)\n",
    "        if not reranker.model:\n",
    "            reranker = None\n",
    "\n",
    "try:\n",
    "    hits = hierarchical_search(\n",
    "        query=query,\n",
    "        indexer=indexer,\n",
    "        embed_mgr=embed_mgr,\n",
    "        model_name=model_name,\n",
    "        reranker=reranker,        \n",
    "        top_k_parent=3,          \n",
    "        top_k_intermediate=5,     \n",
    "        top_k_final=5,           \n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\")\n",
    "    if reranker:\n",
    "        print(f\"--- (Reranked final 128-level with {cfg.reranker_model}) ---\")\n",
    "    else:\n",
    "        print(\"--- (No Reranker Used - Fallback to Vector Search) ---\")\n",
    "\n",
    "    for i, d in enumerate(hits):\n",
    "        print(f\"[{i+1}] {d.metadata.get('source', '?')} | chunk_size={d.metadata.get('chunk_size')}\")\n",
    "        print(f\"  chunk_id={d.metadata.get('chunk_id')}\")\n",
    "        print(f\"  parent_chunk={d.metadata.get('parent_chunk_id')}\")\n",
    "        print(f\"  parent_doc={d.metadata.get('parent_doc_id')}\")\n",
    "        print(\"  \" + d.page_content[:250].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    logger.error(f\"Search failed: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = IngestConfig(\n",
    "    root_dir=Path(r\"C:\\Users\\vsai2\\Documents\\LAMA\\Major Project\\Vals\\Docs\"),\n",
    "    subject=\"Geography - Natural Resources\",\n",
    "    models=(\"all-mpnet-base-v2\",\"BAAI/bge-base-en-v1.5\"), \n",
    "    chunk_sizes=(2048, 512, 128),\n",
    "    reranker_model=None \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 21:12:22,736 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:12:22,737 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2025-10-25 21:12:27,375 [INFO] rag.ingest: No reranker. Using vector similarity for final re-rank.\n",
      "\n",
      "--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\n",
      "--- (No Reranker Used - Fallback to Vector Search) ---\n",
      "[1] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-cbc9641e7d6a10cd\n",
      "  parent_chunk=chunk-068d2c0b45e01b6d\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . physical changes and pollution of land, soil, water and air associated with mining operations, directly or indirectly affects ...\n",
      "\n",
      "[2] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-9a6d12bf11b9688b\n",
      "  parent_chunk=chunk-07f74009d1e0d31c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . d. social impacts associated with large-scale mining activities; result from a rapid influx of workers into mining sites ...\n",
      "\n",
      "[3] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-df1407a7c097d0d0\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . for example mining activity causes removal and destruction of forest resources; contact with toxic soil or water directly ...\n",
      "\n",
      "[4] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-abdd53b6ad19918a\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  water directly kills living organisms; mining operations causes indirect impacts such as change is soil nutrient cycling, ...\n",
      "\n",
      "[5] CHAPTER 2.pdf | chunk_size=128\n",
      "  chunk_id=chunk-f43631e626a8779c\n",
      "  parent_chunk=chunk-00ecdc7ce9c4d67d\n",
      "  parent_doc=doc-da474dca88e2992f\n",
      "  . environmental effects of mineral extraction the scale and level of requirement of minerals have increased manifold in our ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = cfg.models[0]\n",
    "embed_mgr = EmbeddingManager([model_name]) \n",
    "query = \"Impact of mining on the environment\"\n",
    "\n",
    "reranker = None\n",
    "if cfg.reranker_model:\n",
    "    if not _CROSS_ENCODER_OK:\n",
    "        logger.warning(\"CrossEncoder not found. Reranking will be disabled.\")\n",
    "    else:\n",
    "        reranker = Reranker(cfg.reranker_model)\n",
    "        if not reranker.model:\n",
    "            reranker = None \n",
    "\n",
    "try:\n",
    "    hits = hierarchical_search(\n",
    "        query=query,\n",
    "        indexer=indexer,\n",
    "        embed_mgr=embed_mgr,\n",
    "        model_name=model_name,\n",
    "        reranker=reranker,       \n",
    "        top_k_parent=3,           \n",
    "        top_k_intermediate=5,     \n",
    "        top_k_final=5,            \n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\")\n",
    "    if reranker:\n",
    "        print(f\"--- (Reranked final 128-level with {cfg.reranker_model}) ---\")\n",
    "    else:\n",
    "        print(\"--- (No Reranker Used - Fallback to Vector Search) ---\")\n",
    "\n",
    "    for i, d in enumerate(hits):\n",
    "        print(f\"[{i+1}] {d.metadata.get('source', '?')} | chunk_size={d.metadata.get('chunk_size')}\")\n",
    "        print(f\"  chunk_id={d.metadata.get('chunk_id')}\")\n",
    "        print(f\"  parent_chunk={d.metadata.get('parent_chunk_id')}\")\n",
    "        print(f\"  parent_doc={d.metadata.get('parent_doc_id')}\")\n",
    "        print(\"  \" + d.page_content[:250].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    logger.error(f\"Search failed: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d879a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 21:12:38,029 [INFO] sentence_transformers.SentenceTransformer: Use pytorch device_name: cpu\n",
      "2025-10-25 21:12:38,030 [INFO] sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "2025-10-25 21:12:42,294 [INFO] rag.ingest: No reranker. Using vector similarity for final re-rank.\n",
      "\n",
      "--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\n",
      "--- (No Reranker Used - Fallback to Vector Search) ---\n",
      "[1] 1587401677_BA(H)-Psc-Eco-Eng-BA(P)-II-Natural_Resource.pdf | chunk_size=128\n",
      "  chunk_id=chunk-63666a816fd7af7f\n",
      "  parent_chunk=chunk-8aabd4e5f06bcf4e\n",
      "  parent_doc=doc-7b887870a0f03587\n",
      "  . mining major effects of mining operations on forest and tribal people are: • mining from shallow deposits is done by surface ...\n",
      "\n",
      "[2] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-df1407a7c097d0d0\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . for example mining activity causes removal and destruction of forest resources; contact with toxic soil or water directly ...\n",
      "\n",
      "[3] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-2acc9d139ffbc3e5\n",
      "  parent_chunk=chunk-6b6f173f37e914a8\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . removal and disturbance of the land cover and materials in the mining area large scale mining operations disturb the land by ...\n",
      "\n",
      "[4] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-9a6d12bf11b9688b\n",
      "  parent_chunk=chunk-07f74009d1e0d31c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  . d. social impacts associated with large-scale mining activities; result from a rapid influx of workers into mining sites ...\n",
      "\n",
      "[5] Geography of Natural Resource and Management course code GeEs1019.pdf | chunk_size=128\n",
      "  chunk_id=chunk-abdd53b6ad19918a\n",
      "  parent_chunk=chunk-f9b6e5a220e8e44c\n",
      "  parent_doc=doc-a38890677bc460cd\n",
      "  water directly kills living organisms; mining operations causes indirect impacts such as change is soil nutrient cycling, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = cfg.models[1]\n",
    "embed_mgr = EmbeddingManager([model_name]) \n",
    "query = \"Impact of mining on the environment\"\n",
    "\n",
    "reranker = None\n",
    "if cfg.reranker_model:\n",
    "    if not _CROSS_ENCODER_OK:\n",
    "        logger.warning(\"CrossEncoder not found. Reranking will be disabled.\")\n",
    "    else:\n",
    "        reranker = Reranker(cfg.reranker_model)\n",
    "        if not reranker.model:\n",
    "            reranker = None \n",
    "\n",
    "try:\n",
    "    hits = hierarchical_search(\n",
    "        query=query,\n",
    "        indexer=indexer,\n",
    "        embed_mgr=embed_mgr,\n",
    "        model_name=model_name,\n",
    "        reranker=reranker,       \n",
    "        top_k_parent=3,           \n",
    "        top_k_intermediate=5,     \n",
    "        top_k_final=5,           \n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- TOP-DOWN HIERARCHICAL SEARCH (2048->512->128) ---\")\n",
    "    if reranker:\n",
    "        print(f\"--- (Reranked final 128-level with {cfg.reranker_model}) ---\")\n",
    "    else:\n",
    "        print(\"--- (No Reranker Used - Fallback to Vector Search) ---\")\n",
    "\n",
    "    for i, d in enumerate(hits):\n",
    "        print(f\"[{i+1}] {d.metadata.get('source', '?')} | chunk_size={d.metadata.get('chunk_size')}\")\n",
    "        print(f\"  chunk_id={d.metadata.get('chunk_id')}\")\n",
    "        print(f\"  parent_chunk={d.metadata.get('parent_chunk_id')}\")\n",
    "        print(f\"  parent_doc={d.metadata.get('parent_doc_id')}\")\n",
    "        print(\"  \" + d.page_content[:250].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    logger.error(f\"Search failed: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during search: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
